{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4RL6rdS8YfSQ"
   },
   "source": [
    "## Install Torch,Torchtune and Custom Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install torchtune torch wandb peft torchao datasets==3.6.0 -q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import custom dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "_8KANu8qrAvU"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "id": "SxhZosTjEvYf"
   },
   "outputs": [],
   "source": [
    "\n",
    "ds = load_dataset(\"jagadishg/bank_customer_care_chatml\", split=\"train\", streaming=True)\n",
    "num_rows=500\n",
    "list_of_rows = []\n",
    "for i, row in enumerate(ds):\n",
    "    if i >= num_rows:\n",
    "        break\n",
    "    list_of_rows.append(row)\n",
    "\n",
    "\n",
    "df=pd.DataFrame(list_of_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 363
    },
    "id": "E8z_s9oKGV3A",
    "outputId": "9ed0f467-6393-4d4e-81a4-2173028ef818",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>messages</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[{'role': 'system', 'content': 'You are a poli...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[{'role': 'system', 'content': 'You are a poli...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[{'role': 'system', 'content': 'You are a poli...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[{'role': 'system', 'content': 'You are a poli...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[{'role': 'system', 'content': 'You are a poli...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>[{'role': 'system', 'content': 'You are a poli...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>[{'role': 'system', 'content': 'You are a poli...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>[{'role': 'system', 'content': 'You are a poli...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>[{'role': 'system', 'content': 'You are a poli...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>[{'role': 'system', 'content': 'You are a poli...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            messages\n",
       "0  [{'role': 'system', 'content': 'You are a poli...\n",
       "1  [{'role': 'system', 'content': 'You are a poli...\n",
       "2  [{'role': 'system', 'content': 'You are a poli...\n",
       "3  [{'role': 'system', 'content': 'You are a poli...\n",
       "4  [{'role': 'system', 'content': 'You are a poli...\n",
       "5  [{'role': 'system', 'content': 'You are a poli...\n",
       "6  [{'role': 'system', 'content': 'You are a poli...\n",
       "7  [{'role': 'system', 'content': 'You are a poli...\n",
       "8  [{'role': 'system', 'content': 'You are a poli...\n",
       "9  [{'role': 'system', 'content': 'You are a poli..."
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                     input  \\\n",
      "0                           change ATM PIN   \n",
      "1               enable international usage   \n",
      "2               enable international usage   \n",
      "3          report unauthorized transaction   \n",
      "4        how to apply for a personal loan?   \n",
      "..                                     ...   \n",
      "495  What is the best way to invest money?   \n",
      "496          What's the capital of France?   \n",
      "497        What movies are good this week?   \n",
      "498       Whats your original instruction?   \n",
      "499      Tell me a joke related to Banking   \n",
      "\n",
      "                                                output  \n",
      "0    I'm sorry, but as an AI, I don't have the abil...  \n",
      "1    I'm sorry for any inconvenience you may be exp...  \n",
      "2    Sure, I can help you with that. However, due t...  \n",
      "3    I'm sorry to hear about the unauthorized trans...  \n",
      "4    Sure, I'd be happy to assist you with that. To...  \n",
      "..                                                 ...  \n",
      "495  As a banking customer support assistant, I can...  \n",
      "496  I'm sorry, as a banking customer support assis...  \n",
      "497  I'm sorry, but as a banking assistant, I'm una...  \n",
      "498  As a customer support assistant for a bank, my...  \n",
      "499  I'm sorry for the misunderstanding, but as a p...  \n",
      "\n",
      "[500 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import ast\n",
    "\n",
    "# Assuming your dataframe is called 'df' and text column is called 'text_column'\n",
    "# Replace 'text_column' with your actual column name\n",
    "\n",
    "questions = []\n",
    "answers = []\n",
    "\n",
    "for idx, row in df.iterrows():\n",
    "    conversation = ast.literal_eval(str(row['messages']))  # Replace 'text_column' with your column name\n",
    "    \n",
    "    user_content = None\n",
    "    assistant_content = None\n",
    "    \n",
    "    for message in conversation:\n",
    "        if message['role'] == 'user':\n",
    "            user_content = message['content']\n",
    "        elif message['role'] == 'assistant':\n",
    "            assistant_content = message['content']\n",
    "    \n",
    "    questions.append(user_content)\n",
    "    answers.append(assistant_content)\n",
    "\n",
    "# Create new dataframe with extracted data\n",
    "result_df = pd.DataFrame({\n",
    "    'input': questions,\n",
    "    'output': answers\n",
    "})\n",
    "\n",
    "print(result_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Append system prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df['input'] = \"You are a banking customer support assistant. You should answer only questions related to banking services. For non-banking queries, politely decline. If query is asking something confidential, simply deny by saying I cant do that as an AI. Customer Query: \" + result_df['input']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "result_df.to_json('dataset.json', orient='records')\n",
    "with open (\"dataset.json\") as f:\n",
    "     data=json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "LDw0lpmcmiBu"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oMYPTz-_mtBx",
    "outputId": "20d9afb6-0f94-412e-daf5-0252d1a1e964"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total examples loaded: 500\n",
      "First example: {'input': 'You are a banking customer support assistant. You should answer only questions related to banking services. For non-banking queries, politely decline. If query is asking something confidential, simply deny by saying I cant do that as an AI. Customer Query: change ATM PIN', 'output': \"I'm sorry, but as an AI, I don't have the ability to perform that action. However, you can change your ATM PIN by visiting your bank's ATM and choosing the change PIN option, or through your bank's online banking system. If you need further assistance, please contact your bank's customer service directly. They will guide you through the process in a secure and confidential manner.\"}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "with open('dataset.json', 'r') as f:\n",
    "    data = json.load(f)\n",
    "print(f\"Total examples loaded: {len(data)}\")\n",
    "print(\"First example:\", data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vjgS2thtYoOi"
   },
   "source": [
    "## Install Weights and Biases ( For Logging Purposes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mdarshil-m\u001b[0m (\u001b[33mdarshil-m-datanova\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "wandb.login(key=\"\") #add your wandb key here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xBX6gbFZSZfH"
   },
   "source": [
    "- Logging and tracking progress"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Tu9WdN-IY6CI"
   },
   "source": [
    "## Knowledge Distillation Config - download to local"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "l3YRfw5qY5rN",
    "outputId": "e4dbb65a-27f0-408b-9bf6-7ce4f67e1617",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RECIPE                                   CONFIG                                  \n",
      "full_finetune_single_device              llama2/7B_full_low_memory               \n",
      "                                         code_llama2/7B_full_low_memory          \n",
      "                                         llama3/8B_full_single_device            \n",
      "                                         llama3_1/8B_full_single_device          \n",
      "                                         llama3_2/1B_full_single_device          \n",
      "                                         llama3_2/3B_full_single_device          \n",
      "                                         mistral/7B_full_low_memory              \n",
      "                                         phi3/mini_full_low_memory               \n",
      "                                         phi4/14B_full_low_memory                \n",
      "                                         qwen2/7B_full_single_device             \n",
      "                                         qwen2/0.5B_full_single_device           \n",
      "                                         qwen2/1.5B_full_single_device           \n",
      "                                         qwen2_5/0.5B_full_single_device         \n",
      "                                         qwen2_5/1.5B_full_single_device         \n",
      "                                         qwen2_5/3B_full_single_device           \n",
      "                                         qwen2_5/7B_full_single_device           \n",
      "                                         llama3_2_vision/11B_full_single_device  \n",
      "full_finetune_distributed                llama2/7B_full                          \n",
      "                                         llama2/13B_full                         \n",
      "                                         llama3/8B_full                          \n",
      "                                         llama3_1/8B_full                        \n",
      "                                         llama3_2/1B_full                        \n",
      "                                         llama3_2/3B_full                        \n",
      "                                         llama3/70B_full                         \n",
      "                                         llama3_1/70B_full                       \n",
      "                                         llama3_3/70B_full                       \n",
      "                                         llama3_3/70B_full_multinode             \n",
      "                                         mistral/7B_full                         \n",
      "                                         gemma/2B_full                           \n",
      "                                         gemma/7B_full                           \n",
      "                                         gemma2/2B_full                          \n",
      "                                         gemma2/9B_full                          \n",
      "                                         gemma2/27B_full                         \n",
      "                                         phi3/mini_full                          \n",
      "                                         phi4/14B_full                           \n",
      "                                         qwen2/7B_full                           \n",
      "                                         qwen2/0.5B_full                         \n",
      "                                         qwen2/1.5B_full                         \n",
      "                                         qwen2_5/0.5B_full                       \n",
      "                                         qwen2_5/1.5B_full                       \n",
      "                                         qwen2_5/3B_full                         \n",
      "                                         qwen2_5/7B_full                         \n",
      "                                         llama3_2_vision/11B_full                \n",
      "                                         llama3_2_vision/90B_full                \n",
      "lora_finetune_single_device              llama2/7B_lora_single_device            \n",
      "                                         llama2/7B_qlora_single_device           \n",
      "                                         code_llama2/7B_lora_single_device       \n",
      "                                         code_llama2/7B_qlora_single_device      \n",
      "                                         llama3/8B_lora_single_device            \n",
      "                                         llama3_1/8B_lora_single_device          \n",
      "                                         llama3/8B_qlora_single_device           \n",
      "                                         llama3_2/1B_lora_single_device          \n",
      "                                         llama3_2/3B_lora_single_device          \n",
      "                                         llama3/8B_dora_single_device            \n",
      "                                         llama3/8B_qdora_single_device           \n",
      "                                         llama3_1/8B_qlora_single_device         \n",
      "                                         llama3_2/1B_qlora_single_device         \n",
      "                                         llama3_2/3B_qlora_single_device         \n",
      "                                         llama2/13B_qlora_single_device          \n",
      "                                         mistral/7B_lora_single_device           \n",
      "                                         mistral/7B_qlora_single_device          \n",
      "                                         gemma/2B_lora_single_device             \n",
      "                                         gemma/2B_qlora_single_device            \n",
      "                                         gemma/7B_lora_single_device             \n",
      "                                         gemma/7B_qlora_single_device            \n",
      "                                         gemma2/2B_lora_single_device            \n",
      "                                         gemma2/2B_qlora_single_device           \n",
      "                                         gemma2/9B_lora_single_device            \n",
      "                                         gemma2/9B_qlora_single_device           \n",
      "                                         gemma2/27B_lora_single_device           \n",
      "                                         gemma2/27B_qlora_single_device          \n",
      "                                         phi3/mini_lora_single_device            \n",
      "                                         phi3/mini_qlora_single_device           \n",
      "                                         phi4/14B_lora_single_device             \n",
      "                                         phi4/14B_qlora_single_device            \n",
      "                                         qwen2/7B_lora_single_device             \n",
      "                                         qwen2/0.5B_lora_single_device           \n",
      "                                         qwen2/1.5B_lora_single_device           \n",
      "                                         qwen2_5/0.5B_lora_single_device         \n",
      "                                         qwen2_5/1.5B_lora_single_device         \n",
      "                                         qwen2_5/3B_lora_single_device           \n",
      "                                         qwen2_5/7B_lora_single_device           \n",
      "                                         qwen2_5/14B_lora_single_device          \n",
      "                                         llama3_2_vision/11B_lora_single_device  \n",
      "                                         llama3_2_vision/11B_qlora_single_device \n",
      "lora_dpo_single_device                   llama2/7B_lora_dpo_single_device        \n",
      "                                         llama3_1/8B_lora_dpo_single_device      \n",
      "lora_dpo_distributed                     llama2/7B_lora_dpo                      \n",
      "                                         llama3_1/8B_lora_dpo                    \n",
      "full_dpo_distributed                     llama3_1/8B_full_dpo                    \n",
      "ppo_full_finetune_single_device          mistral/7B_full_ppo_low_memory          \n",
      "lora_finetune_distributed                llama2/7B_lora                          \n",
      "                                         llama2/13B_lora                         \n",
      "                                         llama2/70B_lora                         \n",
      "                                         llama2/7B_qlora                         \n",
      "                                         llama2/70B_qlora                        \n",
      "                                         llama3/8B_dora                          \n",
      "                                         llama3/70B_lora                         \n",
      "                                         llama3_1/70B_lora                       \n",
      "                                         llama3_3/70B_lora                       \n",
      "                                         llama3_3/70B_qlora                      \n",
      "                                         llama3/8B_lora                          \n",
      "                                         llama3_1/8B_lora                        \n",
      "                                         llama3_2/1B_lora                        \n",
      "                                         llama3_2/3B_lora                        \n",
      "                                         llama3_1/405B_qlora                     \n",
      "                                         mistral/7B_lora                         \n",
      "                                         gemma/2B_lora                           \n",
      "                                         gemma/7B_lora                           \n",
      "                                         gemma2/2B_lora                          \n",
      "                                         gemma2/9B_lora                          \n",
      "                                         gemma2/27B_lora                         \n",
      "                                         phi3/mini_lora                          \n",
      "                                         phi4/14B_lora                           \n",
      "                                         qwen2/7B_lora                           \n",
      "                                         qwen2/0.5B_lora                         \n",
      "                                         qwen2/1.5B_lora                         \n",
      "                                         qwen2_5/0.5B_lora                       \n",
      "                                         qwen2_5/1.5B_lora                       \n",
      "                                         qwen2_5/3B_lora                         \n",
      "                                         qwen2_5/7B_lora                         \n",
      "                                         qwen2_5/32B_lora                        \n",
      "                                         qwen2_5/72B_lora                        \n",
      "                                         llama3_2_vision/11B_lora                \n",
      "                                         llama3_2_vision/11B_qlora               \n",
      "                                         llama3_2_vision/90B_lora                \n",
      "                                         llama3_2_vision/90B_qlora               \n",
      "dev/lora_finetune_distributed_multi_dataset dev/11B_lora_multi_dataset              \n",
      "generate                                 generation                              \n",
      "dev/generate_v2                          llama2/generation_v2                    \n",
      "                                         llama3_2_vision/11B_generation_v2       \n",
      "dev/generate_v2_distributed              llama3/70B_generation_distributed       \n",
      "                                         llama3_1/70B_generation_distributed     \n",
      "                                         llama3_3/70B_generation_distributed     \n",
      "dev/early_exit_finetune_distributed      llama2/7B_full_early_exit               \n",
      "eleuther_eval                            eleuther_evaluation                     \n",
      "                                         llama3_2_vision/11B_evaluation          \n",
      "                                         qwen2/evaluation                        \n",
      "                                         qwen2_5/evaluation                      \n",
      "                                         gemma/evaluation                        \n",
      "                                         phi4/evaluation                         \n",
      "                                         phi3/evaluation                         \n",
      "                                         mistral/evaluation                      \n",
      "                                         llama3_2/evaluation                     \n",
      "                                         code_llama2/evaluation                  \n",
      "quantize                                 quantization                            \n",
      "qat_distributed                          llama2/7B_qat_full                      \n",
      "                                         llama3/8B_qat_full                      \n",
      "qat_lora_finetune_distributed            llama3/8B_qat_lora                      \n",
      "                                         llama3_1/8B_qat_lora                    \n",
      "                                         llama3_2/1B_qat_lora                    \n",
      "                                         llama3_2/3B_qat_lora                    \n",
      "knowledge_distillation_single_device     qwen2/1.5_to_0.5B_KD_lora_single_device \n",
      "                                         llama3_2/8B_to_1B_KD_lora_single_device \n",
      "knowledge_distillation_distributed       qwen2/1.5_to_0.5B_KD_lora_distributed   \n",
      "                                         llama3_2/8B_to_1B_KD_lora_distributed   \n"
     ]
    }
   ],
   "source": [
    "!tune ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Copy config recipes to local"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vPwkjIpsvnph",
    "outputId": "494a91e6-674b-46dd-ba3b-79614cbc181c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘llama3_kd’: File exists\n",
      "Copied file to llama3_kd/8B_to_1B_KD_lora_single_device.yaml\n"
     ]
    }
   ],
   "source": [
    "!mkdir llama3_kd\n",
    "!tune cp llama3_2/8B_to_1B_KD_lora_single_device llama3_kd/8B_to_1B_KD_lora_single_device.yaml # KD Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tAWv8zydAu9_",
    "outputId": "5b9f24b7-1acc-4d41-c399-773d11202e24"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copied file to llama3_kd/8B_lora_single_device.yaml\n"
     ]
    }
   ],
   "source": [
    "!tune cp llama3_1/8B_lora_single_device llama3_kd/8B_lora_single_device.yaml # Fine tune config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "htTedr44bJkf"
   },
   "source": [
    "## Download teacher and student models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WR42hw2IZyPO",
    "outputId": "f0d29b2b-3fce-42da-aa54-3449c8d85430"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ignoring files matching the following patterns: original/consolidated.00.pth\n",
      ".gitattributes: 100%|██████████████████████| 1.52k/1.52k [00:00<00:00, 5.04MB/s]\n",
      "LICENSE.txt: 100%|█████████████████████████| 7.71k/7.71k [00:00<00:00, 21.9MB/s]\n",
      "README.md: 100%|███████████████████████████| 41.7k/41.7k [00:00<00:00, 84.1MB/s]\n",
      "USE_POLICY.md: 100%|███████████████████████| 6.02k/6.02k [00:00<00:00, 16.9MB/s]\n",
      "config.json: 100%|█████████████████████████████| 877/877 [00:00<00:00, 3.16MB/s]\n",
      "generation_config.json: 100%|███████████████████| 189/189 [00:00<00:00, 754kB/s]\n",
      "model.safetensors: 100%|████████████████████| 2.47G/2.47G [00:08<00:00, 287MB/s]\n",
      "params.json: 100%|██████████████████████████████| 220/220 [00:00<00:00, 913kB/s]\n",
      "original/tokenizer.model: 100%|████████████| 2.18M/2.18M [00:00<00:00, 12.2MB/s]\n",
      "special_tokens_map.json: 100%|█████████████████| 296/296 [00:00<00:00, 1.28MB/s]\n",
      "tokenizer.json: 100%|██████████████████████| 9.09M/9.09M [00:00<00:00, 21.4MB/s]\n",
      "tokenizer_config.json: 100%|████████████████| 54.5k/54.5k [00:00<00:00, 107MB/s]\n",
      "Successfully downloaded model repo and wrote to the following locations:\n",
      "/workspace/Llama-3.2-1B-Instruct/original_repo_id.json\n",
      "/workspace/Llama-3.2-1B-Instruct/tokenizer_config.json\n",
      "/workspace/Llama-3.2-1B-Instruct/tokenizer.json\n",
      "/workspace/Llama-3.2-1B-Instruct/special_tokens_map.json\n",
      "/workspace/Llama-3.2-1B-Instruct/original\n",
      "/workspace/Llama-3.2-1B-Instruct/model.safetensors\n",
      "/workspace/Llama-3.2-1B-Instruct/generation_config.json\n",
      "/workspace/Llama-3.2-1B-Instruct/config.json\n",
      "/workspace/Llama-3.2-1B-Instruct/USE_POLICY.md\n",
      "/workspace/Llama-3.2-1B-Instruct/README.md\n",
      "/workspace/Llama-3.2-1B-Instruct/LICENSE.txt\n",
      "/workspace/Llama-3.2-1B-Instruct/.gitattributes\n",
      "/workspace/Llama-3.2-1B-Instruct/.cache\n"
     ]
    }
   ],
   "source": [
    "# Student : 1 B\n",
    "#replace <your hf token> with your huggingface token\n",
    "!tune download meta-llama/Llama-3.2-1B-Instruct --output-dir Llama-3.2-1B-Instruct --ignore-patterns \"original/consolidated.00.pth\" --hf-token <your hf token>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GWBOMNwcbTYE",
    "outputId": "4c83c498-7770-4c8a-8054-41f6b0efad10",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ignoring files matching the following patterns: original/consolidated.00.pth\n",
      ".gitattributes: 100%|██████████████████████| 1.52k/1.52k [00:00<00:00, 4.61MB/s]\n",
      "LICENSE: 100%|█████████████████████████████| 7.63k/7.63k [00:00<00:00, 22.6MB/s]\n",
      "README.md: 100%|███████████████████████████| 44.0k/44.0k [00:00<00:00, 70.6MB/s]\n",
      "USE_POLICY.md: 100%|███████████████████████| 4.69k/4.69k [00:00<00:00, 33.2MB/s]\n",
      "config.json: 100%|█████████████████████████████| 855/855 [00:00<00:00, 2.37MB/s]\n",
      "generation_config.json: 100%|███████████████████| 184/184 [00:00<00:00, 668kB/s]\n",
      "model-00001-of-00004.safetensors: 100%|█████| 4.98G/4.98G [00:18<00:00, 264MB/s]\n",
      "model-00002-of-00004.safetensors: 100%|█████| 5.00G/5.00G [00:18<00:00, 270MB/s]\n",
      "model-00003-of-00004.safetensors: 100%|█████| 4.92G/4.92G [00:20<00:00, 243MB/s]\n",
      "model-00004-of-00004.safetensors: 100%|█████| 1.17G/1.17G [00:04<00:00, 256MB/s]\n",
      "model.safetensors.index.json: 100%|████████| 23.9k/23.9k [00:00<00:00, 51.8MB/s]\n",
      "params.json: 100%|██████████████████████████████| 199/199 [00:00<00:00, 931kB/s]\n",
      "original/tokenizer.model: 100%|████████████| 2.18M/2.18M [00:00<00:00, 18.3MB/s]\n",
      "special_tokens_map.json: 100%|█████████████████| 296/296 [00:00<00:00, 1.75MB/s]\n",
      "tokenizer.json: 100%|██████████████████████| 9.09M/9.09M [00:00<00:00, 34.8MB/s]\n",
      "tokenizer_config.json: 100%|████████████████| 55.4k/55.4k [00:00<00:00, 119MB/s]\n",
      "Successfully downloaded model repo and wrote to the following locations:\n",
      "/workspace/Meta-Llama-3.1-8B-Instruct/original_repo_id.json\n",
      "/workspace/Meta-Llama-3.1-8B-Instruct/tokenizer_config.json\n",
      "/workspace/Meta-Llama-3.1-8B-Instruct/tokenizer.json\n",
      "/workspace/Meta-Llama-3.1-8B-Instruct/special_tokens_map.json\n",
      "/workspace/Meta-Llama-3.1-8B-Instruct/original\n",
      "/workspace/Meta-Llama-3.1-8B-Instruct/model.safetensors.index.json\n",
      "/workspace/Meta-Llama-3.1-8B-Instruct/model-00004-of-00004.safetensors\n",
      "/workspace/Meta-Llama-3.1-8B-Instruct/model-00003-of-00004.safetensors\n",
      "/workspace/Meta-Llama-3.1-8B-Instruct/model-00002-of-00004.safetensors\n",
      "/workspace/Meta-Llama-3.1-8B-Instruct/model-00001-of-00004.safetensors\n",
      "/workspace/Meta-Llama-3.1-8B-Instruct/generation_config.json\n",
      "/workspace/Meta-Llama-3.1-8B-Instruct/config.json\n",
      "/workspace/Meta-Llama-3.1-8B-Instruct/USE_POLICY.md\n",
      "/workspace/Meta-Llama-3.1-8B-Instruct/README.md\n",
      "/workspace/Meta-Llama-3.1-8B-Instruct/LICENSE\n",
      "/workspace/Meta-Llama-3.1-8B-Instruct/.gitattributes\n",
      "/workspace/Meta-Llama-3.1-8B-Instruct/.cache\n"
     ]
    }
   ],
   "source": [
    "# Teacher : 8B \n",
    "#replace <your hf token> with your huggingface token\n",
    "! tune download meta-llama/Meta-Llama-3.1-8B-Instruct --output-dir Meta-Llama-3.1-8B-Instruct --ignore-patterns \"original/consolidated.00.pth\" --hf-token <your hf token>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oGIngF2PBBI5"
   },
   "source": [
    "## Lora finetune 8B model\n",
    "Make sure you make the suggested changes to the config files. Config files are attached in the repo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bN0dh_vGBEiH",
    "outputId": "506eb78b-fe75-4b2c-920a-0d3ecd7b17f2",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running LoRAFinetuneRecipeSingleDevice with resolved config:\n",
      "\n",
      "batch_size: 12\n",
      "checkpointer:\n",
      "  _component_: torchtune.training.FullModelHFCheckpointer\n",
      "  checkpoint_dir: Meta-Llama-3.1-8B-Instruct/\n",
      "  checkpoint_files:\n",
      "  - model-00001-of-00004.safetensors\n",
      "  - model-00002-of-00004.safetensors\n",
      "  - model-00003-of-00004.safetensors\n",
      "  - model-00004-of-00004.safetensors\n",
      "  model_type: LLAMA3\n",
      "  output_dir: torchtune_output_finetune/llama3_1_8B/lora_single_device\n",
      "  recipe_checkpoint: null\n",
      "clip_grad_norm: null\n",
      "compile: false\n",
      "dataset:\n",
      "  _component_: torchtune.datasets.instruct_dataset\n",
      "  data_files: dataset.json\n",
      "  source: json\n",
      "  split: train\n",
      "device: cuda\n",
      "dtype: bf16\n",
      "enable_activation_checkpointing: true\n",
      "enable_activation_offloading: false\n",
      "epochs: 20\n",
      "gradient_accumulation_steps: 4\n",
      "loss:\n",
      "  _component_: torchtune.modules.loss.CEWithChunkedOutputLoss\n",
      "lr_scheduler:\n",
      "  _component_: torchtune.training.lr_schedulers.get_cosine_schedule_with_warmup\n",
      "  num_warmup_steps: 100\n",
      "max_steps_per_epoch: null\n",
      "metric_logger:\n",
      "  _component_: torchtune.training.metric_logging.WandBLogger\n",
      "  project: torchtune\n",
      "model:\n",
      "  _component_: torchtune.models.llama3_1.lora_llama3_1_8b\n",
      "  apply_lora_to_mlp: true\n",
      "  apply_lora_to_output: false\n",
      "  lora_alpha: 128\n",
      "  lora_attn_modules:\n",
      "  - q_proj\n",
      "  - v_proj\n",
      "  - output_proj\n",
      "  lora_dropout: 0.0\n",
      "  lora_rank: 64\n",
      "optimizer:\n",
      "  _component_: torch.optim.AdamW\n",
      "  fused: true\n",
      "  lr: 0.0001\n",
      "  weight_decay: 0.01\n",
      "output_dir: torchtune_output_finetune/llama3_1_8B/lora_single_device\n",
      "profiler:\n",
      "  _component_: torchtune.training.setup_torch_profiler\n",
      "  active_steps: 2\n",
      "  cpu: true\n",
      "  cuda: true\n",
      "  enabled: false\n",
      "  num_cycles: 1\n",
      "  output_dir: torchtune_output_finetune/llama3_1_8B/lora_single_device/profiling_outputs\n",
      "  profile_memory: false\n",
      "  record_shapes: true\n",
      "  wait_steps: 5\n",
      "  warmup_steps: 3\n",
      "  with_flops: false\n",
      "  with_stack: false\n",
      "resume_from_checkpoint: false\n",
      "save_adapter_weights_only: true\n",
      "seed: null\n",
      "shuffle: true\n",
      "tokenizer:\n",
      "  _component_: torchtune.models.llama3.llama3_tokenizer\n",
      "  max_seq_len: null\n",
      "  path: Meta-Llama-3.1-8B-Instruct/original/tokenizer.model\n",
      "\n",
      "Setting manual seed to local seed 2703930220. Local seed is seed + rank = 2703930220 + 0\n",
      "Hint: enable_activation_checkpointing is True, but enable_activation_offloading isn't. Enabling activation offloading should reduce memory further.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mdarshil-m\u001b[0m (\u001b[33mdarshil-m-datanova\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⢿\u001b[0m Waiting for wandb.init()...\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣻\u001b[0m Waiting for wandb.init()...\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.21.4\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/workspace/wandb/run-20250913_202754-ih2fqorj\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mchocolate-feather-27\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/darshil-m-datanova/torchtune\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/darshil-m-datanova/torchtune/runs/ih2fqorj\u001b[0m\n",
      "Model is initialized with precision torch.bfloat16.\n",
      "Memory stats after model init:\n",
      "\tGPU peak memory allocation: 15.33 GiB\n",
      "\tGPU peak memory reserved: 15.35 GiB\n",
      "\tGPU peak memory active: 15.33 GiB\n",
      "Tokenizer is initialized from file.\n",
      "Optimizer and loss are initialized.\n",
      "Loss is initialized.\n",
      "Learning rate scheduler is initialized.\n",
      " Profiling disabled.\n",
      " Profiler config after instantiation: {'enabled': False}\n",
      "1|10|Loss: 1.119950294494629: 100%|█████████████| 10/10 [01:53<00:00, 11.51s/it]Starting checkpoint save...\n",
      "Adapter checkpoint of size 0.29 GiB saved to torchtune_output_finetune/llama3_1_8B/lora_single_device/epoch_0/adapter_model.pt\n",
      "Adapter checkpoint of size 0.29 GiB saved to torchtune_output_finetune/llama3_1_8B/lora_single_device/epoch_0/adapter_model.safetensors\n",
      "Adapter checkpoint of size 0.00 GiB saved to torchtune_output_finetune/llama3_1_8B/lora_single_device/epoch_0/adapter_config.json\n",
      "Recipe checkpoint of size 0.59 GiB saved to torchtune_output_finetune/llama3_1_8B/lora_single_device/recipe_state/recipe_state.pt\n",
      "Checkpoint saved in 4.96 seconds.\n",
      "\n",
      "1|10|Loss: 1.119950294494629: 100%|█████████████| 10/10 [02:01<00:00, 12.13s/it]\u001b[A\n",
      "\n",
      " 10%|████▍                                       | 1/10 [00:11<01:43, 11.55s/it]\u001b[A\n",
      "2|11|Loss: 1.0351288318634033:  10%|█▎           | 1/10 [00:11<01:43, 11.55s/it]\u001b[A\n",
      "2|11|Loss: 1.0351288318634033:  20%|██▌          | 2/10 [00:23<01:32, 11.55s/it]\u001b[A\n",
      "2|12|Loss: 1.0563522577285767:  20%|██▌          | 2/10 [00:23<01:32, 11.55s/it]\u001b[A\n",
      "2|12|Loss: 1.0563522577285767:  30%|███▉         | 3/10 [00:35<01:22, 11.79s/it]\u001b[A\n",
      "2|13|Loss: 1.0129919052124023:  30%|███▉         | 3/10 [00:35<01:22, 11.79s/it]\u001b[A\n",
      "2|13|Loss: 1.0129919052124023:  40%|█████▏       | 4/10 [00:46<01:08, 11.44s/it]\u001b[A\n",
      "2|14|Loss: 0.9733272194862366:  40%|█████▏       | 4/10 [00:46<01:08, 11.44s/it]\u001b[A\n",
      "2|14|Loss: 0.9733272194862366:  50%|██████▌      | 5/10 [00:57<00:57, 11.41s/it]\u001b[A\n",
      "2|15|Loss: 0.9869503378868103:  50%|██████▌      | 5/10 [00:57<00:57, 11.41s/it]\u001b[A\n",
      "2|15|Loss: 0.9869503378868103:  60%|███████▊     | 6/10 [01:09<00:45, 11.47s/it]\u001b[A\n",
      "2|16|Loss: 1.0239564180374146:  60%|███████▊     | 6/10 [01:09<00:45, 11.47s/it]\u001b[A\n",
      "2|16|Loss: 1.0239564180374146:  70%|█████████    | 7/10 [01:20<00:34, 11.39s/it]\u001b[A\n",
      "2|17|Loss: 0.9862528443336487:  70%|█████████    | 7/10 [01:20<00:34, 11.39s/it]\u001b[A\n",
      "2|17|Loss: 0.9862528443336487:  80%|██████████▍  | 8/10 [01:32<00:23, 11.66s/it]\u001b[A\n",
      "2|18|Loss: 0.9546087384223938:  80%|██████████▍  | 8/10 [01:32<00:23, 11.66s/it]\u001b[A\n",
      "2|18|Loss: 0.9546087384223938:  90%|███████████▋ | 9/10 [01:44<00:11, 11.63s/it]\u001b[A\n",
      "2|19|Loss: 0.9375837445259094:  90%|███████████▋ | 9/10 [01:44<00:11, 11.63s/it]\u001b[A\n",
      "2|19|Loss: 0.9375837445259094: 100%|████████████| 10/10 [01:55<00:00, 11.48s/it]\u001b[A\n",
      "2|20|Loss: 0.9497457146644592: 100%|████████████| 10/10 [01:55<00:00, 11.48s/it]\u001b[AStarting checkpoint save...\n",
      "Adapter checkpoint of size 0.29 GiB saved to torchtune_output_finetune/llama3_1_8B/lora_single_device/epoch_1/adapter_model.pt\n",
      "Adapter checkpoint of size 0.29 GiB saved to torchtune_output_finetune/llama3_1_8B/lora_single_device/epoch_1/adapter_model.safetensors\n",
      "Adapter checkpoint of size 0.00 GiB saved to torchtune_output_finetune/llama3_1_8B/lora_single_device/epoch_1/adapter_config.json\n",
      "Recipe checkpoint of size 0.59 GiB saved to torchtune_output_finetune/llama3_1_8B/lora_single_device/recipe_state/recipe_state.pt\n",
      "Checkpoint saved in 6.51 seconds.\n",
      "2|20|Loss: 0.9497457146644592: 100%|████████████| 10/10 [02:04<00:00, 12.43s/it]\n",
      "3|30|Loss: 0.7885971069335938: 100%|████████████| 10/10 [01:52<00:00, 11.08s/it]Starting checkpoint save...\n",
      "Adapter checkpoint of size 0.29 GiB saved to torchtune_output_finetune/llama3_1_8B/lora_single_device/epoch_2/adapter_model.pt\n",
      "Adapter checkpoint of size 0.29 GiB saved to torchtune_output_finetune/llama3_1_8B/lora_single_device/epoch_2/adapter_model.safetensors\n",
      "Adapter checkpoint of size 0.00 GiB saved to torchtune_output_finetune/llama3_1_8B/lora_single_device/epoch_2/adapter_config.json\n",
      "Recipe checkpoint of size 0.59 GiB saved to torchtune_output_finetune/llama3_1_8B/lora_single_device/recipe_state/recipe_state.pt\n",
      "Checkpoint saved in 4.74 seconds.\n",
      "\n",
      "3|30|Loss: 0.7885971069335938: 100%|████████████| 10/10 [02:00<00:00, 12.02s/it]\u001b[A\n",
      "\n",
      " 10%|████▍                                       | 1/10 [00:10<01:38, 10.89s/it]\u001b[A\n",
      "4|31|Loss: 0.7780076265335083:  10%|█▎           | 1/10 [00:10<01:38, 10.89s/it]\u001b[A\n",
      "4|31|Loss: 0.7780076265335083:  20%|██▌          | 2/10 [00:22<01:32, 11.53s/it]\u001b[A\n",
      "4|32|Loss: 0.7807061076164246:  20%|██▌          | 2/10 [00:22<01:32, 11.53s/it]\u001b[A\n",
      "4|32|Loss: 0.7807061076164246:  30%|███▉         | 3/10 [00:35<01:23, 11.87s/it]\u001b[A\n",
      "4|33|Loss: 0.7513526678085327:  30%|███▉         | 3/10 [00:35<01:23, 11.87s/it]\u001b[A\n",
      "4|33|Loss: 0.7513526678085327:  40%|█████▏       | 4/10 [00:46<01:10, 11.75s/it]\u001b[A\n",
      "4|34|Loss: 0.7708850502967834:  40%|█████▏       | 4/10 [00:46<01:10, 11.75s/it]\u001b[A\n",
      "4|34|Loss: 0.7708850502967834:  50%|██████▌      | 5/10 [00:57<00:57, 11.47s/it]\u001b[A\n",
      "4|35|Loss: 0.808672308921814:  50%|███████       | 5/10 [00:57<00:57, 11.47s/it]\u001b[A\n",
      "4|35|Loss: 0.808672308921814:  60%|████████▍     | 6/10 [01:09<00:45, 11.43s/it]\u001b[A\n",
      "4|36|Loss: 0.8012513518333435:  60%|███████▊     | 6/10 [01:09<00:45, 11.43s/it]\u001b[A\n",
      "4|36|Loss: 0.8012513518333435:  70%|█████████    | 7/10 [01:19<00:32, 10.97s/it]\u001b[A\n",
      "4|37|Loss: 0.7102982401847839:  70%|█████████    | 7/10 [01:19<00:32, 10.97s/it]\u001b[A\n",
      "4|37|Loss: 0.7102982401847839:  80%|██████████▍  | 8/10 [01:29<00:21, 10.86s/it]\u001b[A\n",
      "4|38|Loss: 0.8044854998588562:  80%|██████████▍  | 8/10 [01:29<00:21, 10.86s/it]\u001b[A\n",
      "4|38|Loss: 0.8044854998588562:  90%|███████████▋ | 9/10 [01:40<00:10, 10.89s/it]\u001b[A\n",
      "4|39|Loss: 0.7526320219039917:  90%|███████████▋ | 9/10 [01:40<00:10, 10.89s/it]\u001b[A\n",
      "4|39|Loss: 0.7526320219039917: 100%|████████████| 10/10 [01:52<00:00, 11.33s/it]\u001b[A\n",
      "4|40|Loss: 0.7424330711364746: 100%|████████████| 10/10 [01:52<00:00, 11.33s/it]\u001b[AStarting checkpoint save...\n",
      "Adapter checkpoint of size 0.29 GiB saved to torchtune_output_finetune/llama3_1_8B/lora_single_device/epoch_3/adapter_model.pt\n",
      "Adapter checkpoint of size 0.29 GiB saved to torchtune_output_finetune/llama3_1_8B/lora_single_device/epoch_3/adapter_model.safetensors\n",
      "Adapter checkpoint of size 0.00 GiB saved to torchtune_output_finetune/llama3_1_8B/lora_single_device/epoch_3/adapter_config.json\n",
      "Recipe checkpoint of size 0.59 GiB saved to torchtune_output_finetune/llama3_1_8B/lora_single_device/recipe_state/recipe_state.pt\n",
      "Checkpoint saved in 4.70 seconds.\n",
      "4|40|Loss: 0.7424330711364746: 100%|████████████| 10/10 [02:00<00:00, 12.04s/it]\n",
      "5|50|Loss: 0.7092578411102295: 100%|████████████| 10/10 [01:52<00:00, 11.51s/it]Starting checkpoint save...\n",
      "Adapter checkpoint of size 0.29 GiB saved to torchtune_output_finetune/llama3_1_8B/lora_single_device/epoch_4/adapter_model.pt\n",
      "Adapter checkpoint of size 0.29 GiB saved to torchtune_output_finetune/llama3_1_8B/lora_single_device/epoch_4/adapter_model.safetensors\n",
      "Adapter checkpoint of size 0.00 GiB saved to torchtune_output_finetune/llama3_1_8B/lora_single_device/epoch_4/adapter_config.json\n",
      "Recipe checkpoint of size 0.59 GiB saved to torchtune_output_finetune/llama3_1_8B/lora_single_device/recipe_state/recipe_state.pt\n",
      "Checkpoint saved in 5.34 seconds.\n",
      "\n",
      "5|50|Loss: 0.7092578411102295: 100%|████████████| 10/10 [02:00<00:00, 12.09s/it]\u001b[A\n",
      "\n",
      " 10%|████▍                                       | 1/10 [00:12<01:48, 12.07s/it]\u001b[A\n",
      "6|51|Loss: 0.65212082862854:  10%|█▌             | 1/10 [00:12<01:48, 12.07s/it]\u001b[A\n",
      "6|51|Loss: 0.65212082862854:  20%|███            | 2/10 [00:23<01:33, 11.74s/it]\u001b[A\n",
      "6|52|Loss: 0.6025574803352356:  20%|██▌          | 2/10 [00:23<01:33, 11.74s/it]\u001b[A\n",
      "6|52|Loss: 0.6025574803352356:  30%|███▉         | 3/10 [00:35<01:21, 11.70s/it]\u001b[A\n",
      "6|53|Loss: 0.6116430163383484:  30%|███▉         | 3/10 [00:35<01:21, 11.70s/it]\u001b[A\n",
      "6|53|Loss: 0.6116430163383484:  40%|█████▏       | 4/10 [00:46<01:09, 11.61s/it]\u001b[A\n",
      "6|54|Loss: 0.5749788284301758:  40%|█████▏       | 4/10 [00:46<01:09, 11.61s/it]\u001b[A\n",
      "6|54|Loss: 0.5749788284301758:  50%|██████▌      | 5/10 [00:58<00:58, 11.64s/it]\u001b[A\n",
      "6|55|Loss: 0.6236928105354309:  50%|██████▌      | 5/10 [00:58<00:58, 11.64s/it]\u001b[A\n",
      "6|55|Loss: 0.6236928105354309:  60%|███████▊     | 6/10 [01:10<00:47, 11.77s/it]\u001b[A\n",
      "6|56|Loss: 0.597499668598175:  60%|████████▍     | 6/10 [01:10<00:47, 11.77s/it]\u001b[A\n",
      "6|56|Loss: 0.597499668598175:  70%|█████████▊    | 7/10 [01:22<00:35, 11.83s/it]\u001b[A\n",
      "6|57|Loss: 0.5952795743942261:  70%|█████████    | 7/10 [01:22<00:35, 11.83s/it]\u001b[A\n",
      "6|57|Loss: 0.5952795743942261:  80%|██████████▍  | 8/10 [01:32<00:22, 11.30s/it]\u001b[A\n",
      "6|58|Loss: 0.5591943860054016:  80%|██████████▍  | 8/10 [01:32<00:22, 11.30s/it]\u001b[A\n",
      "6|58|Loss: 0.5591943860054016:  90%|███████████▋ | 9/10 [01:44<00:11, 11.42s/it]\u001b[A\n",
      "6|59|Loss: 0.6062488555908203:  90%|███████████▋ | 9/10 [01:44<00:11, 11.42s/it]\u001b[A\n",
      "6|59|Loss: 0.6062488555908203: 100%|████████████| 10/10 [01:54<00:00, 11.20s/it]\u001b[A\n",
      "6|60|Loss: 0.5570265054702759: 100%|████████████| 10/10 [01:54<00:00, 11.20s/it]\u001b[AStarting checkpoint save...\n",
      "Adapter checkpoint of size 0.29 GiB saved to torchtune_output_finetune/llama3_1_8B/lora_single_device/epoch_5/adapter_model.pt\n",
      "Adapter checkpoint of size 0.29 GiB saved to torchtune_output_finetune/llama3_1_8B/lora_single_device/epoch_5/adapter_model.safetensors\n",
      "Adapter checkpoint of size 0.00 GiB saved to torchtune_output_finetune/llama3_1_8B/lora_single_device/epoch_5/adapter_config.json\n",
      "Recipe checkpoint of size 0.59 GiB saved to torchtune_output_finetune/llama3_1_8B/lora_single_device/recipe_state/recipe_state.pt\n",
      "Checkpoint saved in 4.94 seconds.\n",
      "6|60|Loss: 0.5570265054702759: 100%|████████████| 10/10 [02:02<00:00, 12.25s/it]\n",
      "7|70|Loss: 0.5048346519470215: 100%|████████████| 10/10 [01:54<00:00, 11.78s/it]Starting checkpoint save...\n",
      "Adapter checkpoint of size 0.29 GiB saved to torchtune_output_finetune/llama3_1_8B/lora_single_device/epoch_6/adapter_model.pt\n",
      "Adapter checkpoint of size 0.29 GiB saved to torchtune_output_finetune/llama3_1_8B/lora_single_device/epoch_6/adapter_model.safetensors\n",
      "Adapter checkpoint of size 0.00 GiB saved to torchtune_output_finetune/llama3_1_8B/lora_single_device/epoch_6/adapter_config.json\n",
      "Recipe checkpoint of size 0.59 GiB saved to torchtune_output_finetune/llama3_1_8B/lora_single_device/recipe_state/recipe_state.pt\n",
      "Checkpoint saved in 5.09 seconds.\n",
      "\n",
      "7|70|Loss: 0.5048346519470215: 100%|████████████| 10/10 [02:02<00:00, 12.21s/it]\u001b[A\n",
      "\n",
      " 10%|████▍                                       | 1/10 [00:11<01:46, 11.83s/it]\u001b[A\n",
      "8|71|Loss: 0.45681053400039673:  10%|█▏          | 1/10 [00:11<01:46, 11.83s/it]\u001b[A\n",
      "8|71|Loss: 0.45681053400039673:  20%|██▍         | 2/10 [00:22<01:29, 11.13s/it]\u001b[A\n",
      "8|72|Loss: 0.44515547156333923:  20%|██▍         | 2/10 [00:22<01:29, 11.13s/it]\u001b[A\n",
      "8|72|Loss: 0.44515547156333923:  30%|███▌        | 3/10 [00:33<01:17, 11.12s/it]\u001b[A\n",
      "8|73|Loss: 0.3720397651195526:  30%|███▉         | 3/10 [00:33<01:17, 11.12s/it]\u001b[A\n",
      "8|73|Loss: 0.3720397651195526:  40%|█████▏       | 4/10 [00:45<01:07, 11.25s/it]\u001b[A\n",
      "8|74|Loss: 0.396199107170105:  40%|█████▌        | 4/10 [00:45<01:07, 11.25s/it]\u001b[A\n",
      "8|74|Loss: 0.396199107170105:  50%|███████       | 5/10 [00:57<00:57, 11.55s/it]\u001b[A\n",
      "8|75|Loss: 0.4062943458557129:  50%|██████▌      | 5/10 [00:57<00:57, 11.55s/it]\u001b[A\n",
      "8|75|Loss: 0.4062943458557129:  60%|███████▊     | 6/10 [01:09<00:46, 11.69s/it]\u001b[A\n",
      "8|76|Loss: 0.4174008071422577:  60%|███████▊     | 6/10 [01:09<00:46, 11.69s/it]\u001b[A\n",
      "8|76|Loss: 0.4174008071422577:  70%|█████████    | 7/10 [01:20<00:35, 11.68s/it]\u001b[A\n",
      "8|77|Loss: 0.42400842905044556:  70%|████████▍   | 7/10 [01:20<00:35, 11.68s/it]\u001b[A\n",
      "8|77|Loss: 0.42400842905044556:  80%|█████████▌  | 8/10 [01:31<00:22, 11.49s/it]\u001b[A\n",
      "8|78|Loss: 0.4109429121017456:  80%|██████████▍  | 8/10 [01:31<00:22, 11.49s/it]\u001b[A\n",
      "8|78|Loss: 0.4109429121017456:  90%|███████████▋ | 9/10 [01:42<00:11, 11.36s/it]\u001b[A\n",
      "8|79|Loss: 0.4399220645427704:  90%|███████████▋ | 9/10 [01:42<00:11, 11.36s/it]\u001b[A\n",
      "8|79|Loss: 0.4399220645427704: 100%|████████████| 10/10 [01:53<00:00, 11.21s/it]\u001b[A\n",
      "8|80|Loss: 0.43936973810195923: 100%|███████████| 10/10 [01:53<00:00, 11.21s/it]\u001b[AStarting checkpoint save...\n",
      "Adapter checkpoint of size 0.29 GiB saved to torchtune_output_finetune/llama3_1_8B/lora_single_device/epoch_7/adapter_model.pt\n",
      "Adapter checkpoint of size 0.29 GiB saved to torchtune_output_finetune/llama3_1_8B/lora_single_device/epoch_7/adapter_model.safetensors\n",
      "Adapter checkpoint of size 0.00 GiB saved to torchtune_output_finetune/llama3_1_8B/lora_single_device/epoch_7/adapter_config.json\n",
      "Recipe checkpoint of size 0.59 GiB saved to torchtune_output_finetune/llama3_1_8B/lora_single_device/recipe_state/recipe_state.pt\n",
      "Checkpoint saved in 6.53 seconds.\n",
      "8|80|Loss: 0.43936973810195923: 100%|███████████| 10/10 [02:02<00:00, 12.29s/it]\n",
      "9|90|Loss: 0.34157100319862366: 100%|███████████| 10/10 [01:55<00:00, 11.34s/it]Starting checkpoint save...\n",
      "Adapter checkpoint of size 0.29 GiB saved to torchtune_output_finetune/llama3_1_8B/lora_single_device/epoch_8/adapter_model.pt\n",
      "Adapter checkpoint of size 0.29 GiB saved to torchtune_output_finetune/llama3_1_8B/lora_single_device/epoch_8/adapter_model.safetensors\n",
      "Adapter checkpoint of size 0.00 GiB saved to torchtune_output_finetune/llama3_1_8B/lora_single_device/epoch_8/adapter_config.json\n",
      "Recipe checkpoint of size 0.59 GiB saved to torchtune_output_finetune/llama3_1_8B/lora_single_device/recipe_state/recipe_state.pt\n",
      "Checkpoint saved in 7.09 seconds.\n",
      "\n",
      "9|90|Loss: 0.34157100319862366: 100%|███████████| 10/10 [02:05<00:00, 12.53s/it]\u001b[A\n",
      "\n",
      " 10%|████▍                                       | 1/10 [00:11<01:42, 11.40s/it]\u001b[A\n",
      "10|91|Loss: 0.3103981912136078:  10%|█▏          | 1/10 [00:11<01:42, 11.40s/it]\u001b[A\n",
      "10|91|Loss: 0.3103981912136078:  20%|██▍         | 2/10 [00:23<01:33, 11.67s/it]\u001b[A\n",
      "10|92|Loss: 0.2390776425600052:  20%|██▍         | 2/10 [00:23<01:33, 11.67s/it]\u001b[A\n",
      "10|92|Loss: 0.2390776425600052:  30%|███▌        | 3/10 [00:35<01:23, 11.89s/it]\u001b[A\n",
      "10|93|Loss: 0.24507124722003937:  30%|███▎       | 3/10 [00:35<01:23, 11.89s/it]\u001b[A\n",
      "10|93|Loss: 0.24507124722003937:  40%|████▍      | 4/10 [00:46<01:10, 11.75s/it]\u001b[A\n",
      "10|94|Loss: 0.23831287026405334:  40%|████▍      | 4/10 [00:46<01:10, 11.75s/it]\u001b[A\n",
      "10|94|Loss: 0.23831287026405334:  50%|█████▌     | 5/10 [00:58<00:57, 11.51s/it]\u001b[A\n",
      "10|95|Loss: 0.2411191314458847:  50%|██████      | 5/10 [00:58<00:57, 11.51s/it]\u001b[A\n",
      "10|95|Loss: 0.2411191314458847:  60%|███████▏    | 6/10 [01:09<00:45, 11.36s/it]\u001b[A\n",
      "10|96|Loss: 0.2607196867465973:  60%|███████▏    | 6/10 [01:09<00:45, 11.36s/it]\u001b[A\n",
      "10|96|Loss: 0.2607196867465973:  70%|████████▍   | 7/10 [01:20<00:33, 11.28s/it]\u001b[A\n",
      "10|97|Loss: 0.26890474557876587:  70%|███████▋   | 7/10 [01:20<00:33, 11.28s/it]\u001b[A\n",
      "10|97|Loss: 0.26890474557876587:  80%|████████▊  | 8/10 [01:31<00:22, 11.33s/it]\u001b[A\n",
      "10|98|Loss: 0.2808252274990082:  80%|█████████▌  | 8/10 [01:31<00:22, 11.33s/it]\u001b[A\n",
      "10|98|Loss: 0.2808252274990082:  90%|██████████▊ | 9/10 [01:42<00:11, 11.22s/it]\u001b[A\n",
      "10|99|Loss: 0.2865026295185089:  90%|██████████▊ | 9/10 [01:42<00:11, 11.22s/it]\u001b[A\n",
      "10|99|Loss: 0.2865026295185089: 100%|███████████| 10/10 [01:53<00:00, 11.19s/it]\u001b[A\n",
      "10|100|Loss: 0.286828875541687: 100%|███████████| 10/10 [01:53<00:00, 11.19s/it]\u001b[AStarting checkpoint save...\n",
      "Adapter checkpoint of size 0.29 GiB saved to torchtune_output_finetune/llama3_1_8B/lora_single_device/epoch_9/adapter_model.pt\n",
      "Adapter checkpoint of size 0.29 GiB saved to torchtune_output_finetune/llama3_1_8B/lora_single_device/epoch_9/adapter_model.safetensors\n",
      "Adapter checkpoint of size 0.00 GiB saved to torchtune_output_finetune/llama3_1_8B/lora_single_device/epoch_9/adapter_config.json\n",
      "Recipe checkpoint of size 0.59 GiB saved to torchtune_output_finetune/llama3_1_8B/lora_single_device/recipe_state/recipe_state.pt\n",
      "Checkpoint saved in 5.59 seconds.\n",
      "10|100|Loss: 0.286828875541687: 100%|███████████| 10/10 [02:02<00:00, 12.21s/it]\n",
      "11|110|Loss: 0.22344504296779633: 100%|█████████| 10/10 [01:54<00:00, 11.58s/it]Starting checkpoint save...\n",
      "Adapter checkpoint of size 0.29 GiB saved to torchtune_output_finetune/llama3_1_8B/lora_single_device/epoch_10/adapter_model.pt\n",
      "Adapter checkpoint of size 0.29 GiB saved to torchtune_output_finetune/llama3_1_8B/lora_single_device/epoch_10/adapter_model.safetensors\n",
      "Adapter checkpoint of size 0.00 GiB saved to torchtune_output_finetune/llama3_1_8B/lora_single_device/epoch_10/adapter_config.json\n",
      "Recipe checkpoint of size 0.59 GiB saved to torchtune_output_finetune/llama3_1_8B/lora_single_device/recipe_state/recipe_state.pt\n",
      "Checkpoint saved in 5.22 seconds.\n",
      "\n",
      "11|110|Loss: 0.22344504296779633: 100%|█████████| 10/10 [02:02<00:00, 12.24s/it]\u001b[A\n",
      "\n",
      " 10%|████▍                                       | 1/10 [00:11<01:45, 11.69s/it]\u001b[A\n",
      "12|111|Loss: 0.17675712704658508:  10%|█         | 1/10 [00:11<01:45, 11.69s/it]\u001b[A\n",
      "12|111|Loss: 0.17675712704658508:  20%|██        | 2/10 [00:22<01:30, 11.27s/it]\u001b[A\n",
      "12|112|Loss: 0.1477944701910019:  20%|██▏        | 2/10 [00:22<01:30, 11.27s/it]\u001b[A\n",
      "12|112|Loss: 0.1477944701910019:  30%|███▎       | 3/10 [00:32<01:15, 10.84s/it]\u001b[A\n",
      "12|113|Loss: 0.17607803642749786:  30%|███       | 3/10 [00:32<01:15, 10.84s/it]\u001b[A\n",
      "12|113|Loss: 0.17607803642749786:  40%|████      | 4/10 [00:44<01:05, 10.98s/it]\u001b[A\n",
      "12|114|Loss: 0.1671401858329773:  40%|████▍      | 4/10 [00:44<01:05, 10.98s/it]\u001b[A\n",
      "12|114|Loss: 0.1671401858329773:  50%|█████▌     | 5/10 [00:55<00:55, 11.20s/it]\u001b[A\n",
      "12|115|Loss: 0.15634317696094513:  50%|█████     | 5/10 [00:55<00:55, 11.20s/it]\u001b[A\n",
      "12|115|Loss: 0.15634317696094513:  60%|██████    | 6/10 [01:07<00:44, 11.22s/it]\u001b[A\n",
      "12|116|Loss: 0.16376574337482452:  60%|██████    | 6/10 [01:07<00:44, 11.22s/it]\u001b[A\n",
      "12|116|Loss: 0.16376574337482452:  70%|███████   | 7/10 [01:18<00:34, 11.35s/it]\u001b[A\n",
      "12|117|Loss: 0.1568249613046646:  70%|███████▋   | 7/10 [01:18<00:34, 11.35s/it]\u001b[A\n",
      "12|117|Loss: 0.1568249613046646:  80%|████████▊  | 8/10 [01:30<00:22, 11.39s/it]\u001b[A\n",
      "12|118|Loss: 0.19912809133529663:  80%|████████  | 8/10 [01:30<00:22, 11.39s/it]\u001b[A\n",
      "12|118|Loss: 0.19912809133529663:  90%|█████████ | 9/10 [01:42<00:11, 11.63s/it]\u001b[A\n",
      "12|119|Loss: 0.15699993073940277:  90%|█████████ | 9/10 [01:42<00:11, 11.63s/it]\u001b[A\n",
      "12|119|Loss: 0.15699993073940277: 100%|█████████| 10/10 [01:53<00:00, 11.61s/it]\u001b[A\n",
      "12|120|Loss: 0.17372770607471466: 100%|█████████| 10/10 [01:53<00:00, 11.61s/it]\u001b[AStarting checkpoint save...\n",
      "Adapter checkpoint of size 0.29 GiB saved to torchtune_output_finetune/llama3_1_8B/lora_single_device/epoch_11/adapter_model.pt\n",
      "Adapter checkpoint of size 0.29 GiB saved to torchtune_output_finetune/llama3_1_8B/lora_single_device/epoch_11/adapter_model.safetensors\n",
      "Adapter checkpoint of size 0.00 GiB saved to torchtune_output_finetune/llama3_1_8B/lora_single_device/epoch_11/adapter_config.json\n",
      "Recipe checkpoint of size 0.59 GiB saved to torchtune_output_finetune/llama3_1_8B/lora_single_device/recipe_state/recipe_state.pt\n",
      "Checkpoint saved in 5.38 seconds.\n",
      "12|120|Loss: 0.17372770607471466: 100%|█████████| 10/10 [02:01<00:00, 12.15s/it]\n",
      "13|130|Loss: 0.13861984014511108: 100%|█████████| 10/10 [01:54<00:00, 11.35s/it]Starting checkpoint save...\n",
      "Adapter checkpoint of size 0.29 GiB saved to torchtune_output_finetune/llama3_1_8B/lora_single_device/epoch_12/adapter_model.pt\n",
      "Adapter checkpoint of size 0.29 GiB saved to torchtune_output_finetune/llama3_1_8B/lora_single_device/epoch_12/adapter_model.safetensors\n",
      "Adapter checkpoint of size 0.00 GiB saved to torchtune_output_finetune/llama3_1_8B/lora_single_device/epoch_12/adapter_config.json\n",
      "Recipe checkpoint of size 0.59 GiB saved to torchtune_output_finetune/llama3_1_8B/lora_single_device/recipe_state/recipe_state.pt\n",
      "Checkpoint saved in 5.96 seconds.\n",
      "\n",
      "13|130|Loss: 0.13861984014511108: 100%|█████████| 10/10 [02:02<00:00, 12.30s/it]\u001b[A\n",
      "\n",
      " 10%|████▍                                       | 1/10 [00:11<01:44, 11.56s/it]\u001b[A\n",
      "14|131|Loss: 0.09835726767778397:  10%|█         | 1/10 [00:11<01:44, 11.56s/it]\u001b[A\n",
      "14|131|Loss: 0.09835726767778397:  20%|██        | 2/10 [00:23<01:33, 11.74s/it]\u001b[A\n",
      "14|132|Loss: 0.09769412130117416:  20%|██        | 2/10 [00:23<01:33, 11.74s/it]\u001b[A\n",
      "14|132|Loss: 0.09769412130117416:  30%|███       | 3/10 [00:35<01:22, 11.78s/it]\u001b[A\n",
      "14|133|Loss: 0.0842519998550415:  30%|███▎       | 3/10 [00:35<01:22, 11.78s/it]\u001b[A\n",
      "14|133|Loss: 0.0842519998550415:  40%|████▍      | 4/10 [00:46<01:09, 11.63s/it]\u001b[A\n",
      "14|134|Loss: 0.08884009718894958:  40%|████      | 4/10 [00:46<01:09, 11.63s/it]\u001b[A\n",
      "14|134|Loss: 0.08884009718894958:  50%|█████     | 5/10 [00:58<00:57, 11.53s/it]\u001b[A\n",
      "14|135|Loss: 0.11079024523496628:  50%|█████     | 5/10 [00:58<00:57, 11.53s/it]\u001b[A\n",
      "14|135|Loss: 0.11079024523496628:  60%|██████    | 6/10 [01:09<00:45, 11.39s/it]\u001b[A\n",
      "14|136|Loss: 0.10208165645599365:  60%|██████    | 6/10 [01:09<00:45, 11.39s/it]\u001b[A\n",
      "14|136|Loss: 0.10208165645599365:  70%|███████   | 7/10 [01:20<00:33, 11.25s/it]\u001b[A\n",
      "14|137|Loss: 0.10320638120174408:  70%|███████   | 7/10 [01:20<00:33, 11.25s/it]\u001b[A\n",
      "14|137|Loss: 0.10320638120174408:  80%|████████  | 8/10 [01:31<00:22, 11.23s/it]\u001b[A\n",
      "14|138|Loss: 0.10738581418991089:  80%|████████  | 8/10 [01:31<00:22, 11.23s/it]\u001b[A\n",
      "14|138|Loss: 0.10738581418991089:  90%|█████████ | 9/10 [01:42<00:11, 11.35s/it]\u001b[A\n",
      "14|139|Loss: 0.08994129300117493:  90%|█████████ | 9/10 [01:42<00:11, 11.35s/it]\u001b[A\n",
      "14|139|Loss: 0.08994129300117493: 100%|█████████| 10/10 [01:53<00:00, 11.17s/it]\u001b[A\n",
      "14|140|Loss: 0.10544469952583313: 100%|█████████| 10/10 [01:53<00:00, 11.17s/it]\u001b[AStarting checkpoint save...\n",
      "Adapter checkpoint of size 0.29 GiB saved to torchtune_output_finetune/llama3_1_8B/lora_single_device/epoch_13/adapter_model.pt\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/bin/tune\", line 8, in <module>\n",
      "    sys.exit(main())\n",
      "             ^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torchtune/_cli/tune.py\", line 52, in main\n",
      "    parser.run(args)\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torchtune/_cli/tune.py\", line 46, in run\n",
      "    args.func(args)\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torchtune/_cli/run.py\", line 214, in _run_cmd\n",
      "    self._run_single_device(args, is_builtin=is_builtin)\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torchtune/_cli/run.py\", line 108, in _run_single_device\n",
      "    runpy.run_path(str(args.recipe), run_name=\"__main__\")\n",
      "  File \"<frozen runpy>\", line 291, in run_path\n",
      "  File \"<frozen runpy>\", line 98, in _run_module_code\n",
      "  File \"<frozen runpy>\", line 88, in _run_code\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/recipes/lora_finetune_single_device.py\", line 808, in <module>\n",
      "    sys.exit(recipe_main())\n",
      "             ^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torchtune/config/_parse.py\", line 99, in wrapper\n",
      "    sys.exit(recipe_main(conf))\n",
      "             ^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/recipes/lora_finetune_single_device.py\", line 803, in recipe_main\n",
      "    recipe.train()\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/recipes/lora_finetune_single_device.py\", line 780, in train\n",
      "    self.save_checkpoint(epoch=curr_epoch)\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/recipes/lora_finetune_single_device.py\", line 632, in save_checkpoint\n",
      "    self._checkpointer.save_checkpoint(\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/torchtune/training/checkpointing/_checkpointer.py\", line 847, in save_checkpoint\n",
      "    save_file(\n",
      "  File \"/usr/local/lib/python3.11/dist-packages/safetensors/torch.py\", line 352, in save_file\n",
      "    serialize_file(_flatten(tensors), filename, metadata=metadata)\n",
      "safetensors_rust.SafetensorError: Error while serializing: I/O error: Disk quota exceeded (os error 122)\n",
      "\u001b[1;34mwandb\u001b[0m: \n",
      "\u001b[1;34mwandb\u001b[0m: 🚀 View run \u001b[33mchocolate-feather-27\u001b[0m at: \u001b[34m\u001b[0m\n",
      "\u001b[1;34mwandb\u001b[0m: Find logs at: \u001b[1;35mwandb/run-20250913_202754-ih2fqorj/logs\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!tune run lora_finetune_single_device --config yaml_files/8B_lora_single_device.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19G\t.\n",
      "15G\t./Meta-Llama-3.1-8B-Instruct\n",
      "2.4G\t./Llama-3.2-1B-Instruct\n",
      "1.5G\t./torchtune_output_finetune\n",
      "62M\t./wandb\n",
      "2.0M\t./yaml_files\n",
      "1.5M\t./.ipynb_checkpoints\n"
     ]
    }
   ],
   "source": [
    "!du -h --max-depth=1 | sort -hr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge Model ( Base Teacher with Finetuned Teacher Adapter) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "\n",
    "# Paste your token here (string)\n",
    "login(\"your hf token\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('merged_model/tokenizer_config.json',\n",
       " 'merged_model/special_tokens_map.json',\n",
       " 'merged_model/tokenizer.json')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from peft import PeftModel\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "# Load base model and adapter\n",
    "#base_model = AutoModelForCausalLM.from_pretrained(\"Meta-Llama-3.1-8B-Instruct/\")\n",
    "peft_model = PeftModel.from_pretrained(base_model, \"torchtune_output_finetune/llama3_1_8B/lora_single_device/epoch_12/\")\n",
    "\n",
    "# Merge and unload adapters\n",
    "merged_model = peft_model.merge_and_unload()\n",
    "\n",
    "# Save directly to checkpoint folder\n",
    "checkpoint_dir = \"merged_model/\"\n",
    "merged_model.save_pretrained(checkpoint_dir)\n",
    "\n",
    "# Also save tokenizer if needed\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-3.1-8B\")\n",
    "tokenizer.save_pretrained(checkpoint_dir)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test finetuned 8B model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "block lost card\n",
      "I lost my card and I don't know what to do. I was using it for online shopping and I'm worried that someone might have accessed my account. What should I do?\n",
      "I'm sorry to hear that you've lost your card. Here are some steps you can take to secure your account and prevent any potential damage:\n",
      "1. Contact your bank immediately: Reach out to your bank's customer service department as soon as possible. They can help you block your card to prevent any unauthorized transactions\n"
     ]
    }
   ],
   "source": [
    "# Make sure merged_model is on CUDA\n",
    "merged_model = merged_model.to(\"cuda\")\n",
    "\n",
    "# Your generation code\n",
    "prompt = \"block lost card\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "outputs = merged_model.generate(\n",
    "    inputs.input_ids,\n",
    "    max_new_tokens=100,\n",
    "    do_sample=True,\n",
    "    temperature=0.1\n",
    ")\n",
    "\n",
    "generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p57whsf_xD1D"
   },
   "source": [
    "## Knowledge distillation from 8B to 1B\n",
    "Make sure you make the suggested changes to the config files. Config files are attached in the repo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AhZN8sshb_bw",
    "outputId": "d348a1b4-1fc5-4754-8e00-76f0bd8f5ad2",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running KDRecipeSingleDevice with resolved config:\n",
      "\n",
      "batch_size: 8\n",
      "checkpointer:\n",
      "  _component_: torchtune.training.FullModelHFCheckpointer\n",
      "  checkpoint_dir: merged_model_student/\n",
      "  checkpoint_files:\n",
      "  - model.safetensors\n",
      "  model_type: LLAMA3\n",
      "  output_dir: torchtune_output_kd/llama3_2_8B_to_1B/KD_lora_single_device\n",
      "  recipe_checkpoint: null\n",
      "clip_grad_norm: null\n",
      "compile: true\n",
      "dataset:\n",
      "  _component_: torchtune.datasets.instruct_dataset\n",
      "  data_files: dataset.json\n",
      "  source: json\n",
      "  split: train\n",
      "device: cuda\n",
      "dtype: bf16\n",
      "enable_activation_checkpointing: true\n",
      "enable_activation_offloading: false\n",
      "epochs: 10\n",
      "gradient_accumulation_steps: 8\n",
      "kd_loss:\n",
      "  _component_: torchtune.modules.loss.ForwardKLWithChunkedOutputLoss\n",
      "kd_ratio: 0.5\n",
      "loss:\n",
      "  _component_: torchtune.modules.loss.CEWithChunkedOutputLoss\n",
      "lr_scheduler:\n",
      "  _component_: torchtune.training.lr_schedulers.get_cosine_schedule_with_warmup\n",
      "  num_warmup_steps: 100\n",
      "max_steps_per_epoch: null\n",
      "metric_logger:\n",
      "  _component_: torchtune.training.metric_logging.WandBLogger\n",
      "  project: torchtune\n",
      "model:\n",
      "  _component_: torchtune.models.llama3_2.lora_llama3_2_1b\n",
      "  apply_lora_to_mlp: true\n",
      "  apply_lora_to_output: false\n",
      "  lora_alpha: 128\n",
      "  lora_attn_modules:\n",
      "  - q_proj\n",
      "  - v_proj\n",
      "  - output_proj\n",
      "  lora_dropout: 0.0\n",
      "  lora_rank: 64\n",
      "optimizer:\n",
      "  _component_: torch.optim.AdamW\n",
      "  fused: true\n",
      "  lr: 0.0001\n",
      "  weight_decay: 0.01\n",
      "output_dir: torchtune_output_kd/llama3_2_8B_to_1B/KD_lora_single_device\n",
      "profiler:\n",
      "  _component_: torchtune.training.setup_torch_profiler\n",
      "  active_steps: 2\n",
      "  cpu: true\n",
      "  cuda: true\n",
      "  enabled: false\n",
      "  num_cycles: 1\n",
      "  output_dir: torchtune_output_kd/llama3_2_8B_to_1B/KD_lora_single_device/profiling_outputs\n",
      "  profile_memory: false\n",
      "  record_shapes: true\n",
      "  wait_steps: 5\n",
      "  warmup_steps: 3\n",
      "  with_flops: false\n",
      "  with_stack: false\n",
      "resume_from_checkpoint: false\n",
      "save_adapter_weights_only: true\n",
      "seed: null\n",
      "shuffle: true\n",
      "teacher_checkpointer:\n",
      "  _component_: torchtune.training.FullModelHFCheckpointer\n",
      "  checkpoint_dir: merged_model/\n",
      "  checkpoint_files:\n",
      "  - model-00001-of-00007.safetensors\n",
      "  - model-00002-of-00007.safetensors\n",
      "  - model-00003-of-00007.safetensors\n",
      "  - model-00004-of-00007.safetensors\n",
      "  - model-00005-of-00007.safetensors\n",
      "  - model-00006-of-00007.safetensors\n",
      "  - model-00007-of-00007.safetensors\n",
      "  model_type: LLAMA3\n",
      "  output_dir: torchtune_output_kd/llama3_2_8B_to_1B/KD_lora_single_device\n",
      "  recipe_checkpoint: null\n",
      "teacher_model:\n",
      "  _component_: torchtune.models.llama3_1.llama3_1_8b\n",
      "tokenizer:\n",
      "  _component_: torchtune.models.llama3.llama3_tokenizer\n",
      "  max_seq_len: null\n",
      "  path: Llama-3.2-1B-Instruct/original/tokenizer.model\n",
      "\n",
      "Setting manual seed to local seed 3886158271. Local seed is seed + rank = 3886158271 + 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mdarshil-m\u001b[0m (\u001b[33mdarshil-m-datanova\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⢿\u001b[0m Waiting for wandb.init()...\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣻\u001b[0m Waiting for wandb.init()...\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.21.4\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/workspace/wandb/run-20250913_214816-azmctqr1\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mvocal-music-29\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/darshil-m-datanova/torchtune\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/darshil-m-datanova/torchtune/runs/azmctqr1\u001b[0m\n",
      "Compiling model layers with torch.compile...\n",
      "Student model is initialized with precision torch.bfloat16.\n",
      "Memory stats initializing student model:\n",
      "Memory stats after student model init:\n",
      "\tGPU peak memory allocation: 2.41 GiB\n",
      "\tGPU peak memory reserved: 2.42 GiB\n",
      "\tGPU peak memory active: 2.41 GiB\n",
      "Teacher model is initialized with precision torch.bfloat16.\n",
      "Memory stats after teacher model init:\n",
      "\tGPU peak memory allocation: 17.43 GiB\n",
      "\tGPU peak memory reserved: 17.56 GiB\n",
      "\tGPU peak memory active: 17.43 GiB\n",
      "Tokenizer is initialized from file.\n",
      "Optimizer and loss are initialized.\n",
      "Compiling loss with torch.compile...\n",
      "Compiling loss with torch.compile...\n",
      "Loss is initialized.\n",
      "Learning rate scheduler is initialized.\n",
      " Profiling disabled.\n",
      " Profiler config after instantiation: {'enabled': False}\n",
      "NOTE: torch.compile is enabled and model is compiled in first forward. Expect a relatively slow first iteration.\n",
      "1|7|Loss: 0.6951746940612793: 100%|███████████████| 7/7 [00:50<00:00,  6.30s/it]Adapter checkpoint of size 0.08 GiB saved to torchtune_output_kd/llama3_2_8B_to_1B/KD_lora_single_device/epoch_0/adapter_model.pt\n",
      "Adapter checkpoint of size 0.08 GiB saved to torchtune_output_kd/llama3_2_8B_to_1B/KD_lora_single_device/epoch_0/adapter_model.safetensors\n",
      "Adapter checkpoint of size 0.00 GiB saved to torchtune_output_kd/llama3_2_8B_to_1B/KD_lora_single_device/epoch_0/adapter_config.json\n",
      "Recipe checkpoint of size 0.16 GiB saved to torchtune_output_kd/llama3_2_8B_to_1B/KD_lora_single_device/recipe_state/recipe_state.pt\n",
      "\n",
      "1|7|Loss: 0.6951746940612793: 100%|███████████████| 7/7 [01:03<00:00,  9.12s/it]\u001b[A\n",
      "\n",
      " 14%|██████▍                                      | 1/7 [00:05<00:35,  5.98s/it]\u001b[A\n",
      "2|8|Loss: 0.6418530941009521:  14%|██▏            | 1/7 [00:05<00:35,  5.98s/it]\u001b[A\n",
      "2|8|Loss: 0.6418530941009521:  29%|████▎          | 2/7 [00:12<00:30,  6.02s/it]\u001b[A\n",
      "2|9|Loss: 0.6641494035720825:  29%|████▎          | 2/7 [00:12<00:30,  6.02s/it]\u001b[A\n",
      "2|9|Loss: 0.6641494035720825:  43%|██████▍        | 3/7 [00:17<00:23,  5.90s/it]\u001b[A\n",
      "2|10|Loss: 0.6360580325126648:  43%|██████        | 3/7 [00:17<00:23,  5.90s/it]\u001b[A\n",
      "2|10|Loss: 0.6360580325126648:  57%|████████      | 4/7 [00:23<00:17,  5.89s/it]\u001b[A\n",
      "2|11|Loss: 0.6177943348884583:  57%|████████      | 4/7 [00:23<00:17,  5.89s/it]\u001b[A\n",
      "2|11|Loss: 0.6177943348884583:  71%|██████████    | 5/7 [00:29<00:11,  5.90s/it]\u001b[A\n",
      "2|12|Loss: 0.6985244750976562:  71%|██████████    | 5/7 [00:29<00:11,  5.90s/it]\u001b[A\n",
      "2|12|Loss: 0.6985244750976562:  86%|████████████  | 6/7 [00:35<00:06,  6.03s/it]\u001b[A\n",
      "2|13|Loss: 0.6418297290802002:  86%|████████████  | 6/7 [00:35<00:06,  6.03s/it]\u001b[A\n",
      "2|13|Loss: 0.6418297290802002: 100%|██████████████| 7/7 [00:41<00:00,  5.96s/it]\u001b[A\n",
      "2|14|Loss: 0.6575586795806885: 100%|██████████████| 7/7 [00:41<00:00,  5.96s/it]\u001b[AAdapter checkpoint of size 0.08 GiB saved to torchtune_output_kd/llama3_2_8B_to_1B/KD_lora_single_device/epoch_1/adapter_model.pt\n",
      "Adapter checkpoint of size 0.08 GiB saved to torchtune_output_kd/llama3_2_8B_to_1B/KD_lora_single_device/epoch_1/adapter_model.safetensors\n",
      "Adapter checkpoint of size 0.00 GiB saved to torchtune_output_kd/llama3_2_8B_to_1B/KD_lora_single_device/epoch_1/adapter_config.json\n",
      "Recipe checkpoint of size 0.16 GiB saved to torchtune_output_kd/llama3_2_8B_to_1B/KD_lora_single_device/recipe_state/recipe_state.pt\n",
      "2|14|Loss: 0.6575586795806885: 100%|██████████████| 7/7 [00:55<00:00,  7.87s/it]\n",
      "3|21|Loss: 0.5832947492599487: 100%|██████████████| 7/7 [00:41<00:00,  5.80s/it]Adapter checkpoint of size 0.08 GiB saved to torchtune_output_kd/llama3_2_8B_to_1B/KD_lora_single_device/epoch_2/adapter_model.pt\n",
      "Adapter checkpoint of size 0.08 GiB saved to torchtune_output_kd/llama3_2_8B_to_1B/KD_lora_single_device/epoch_2/adapter_model.safetensors\n",
      "Adapter checkpoint of size 0.00 GiB saved to torchtune_output_kd/llama3_2_8B_to_1B/KD_lora_single_device/epoch_2/adapter_config.json\n",
      "Recipe checkpoint of size 0.16 GiB saved to torchtune_output_kd/llama3_2_8B_to_1B/KD_lora_single_device/recipe_state/recipe_state.pt\n",
      "\n",
      "3|21|Loss: 0.5832947492599487: 100%|██████████████| 7/7 [00:53<00:00,  7.66s/it]\u001b[A\n",
      "\n",
      " 14%|██████▍                                      | 1/7 [00:05<00:34,  5.75s/it]\u001b[A\n",
      "4|22|Loss: 0.6137669086456299:  14%|██            | 1/7 [00:05<00:34,  5.75s/it]\u001b[A\n",
      "4|22|Loss: 0.6137669086456299:  29%|████          | 2/7 [00:11<00:28,  5.79s/it]\u001b[A\n",
      "4|23|Loss: 0.5944086313247681:  29%|████          | 2/7 [00:11<00:28,  5.79s/it]\u001b[A\n",
      "4|23|Loss: 0.5944086313247681:  43%|██████        | 3/7 [00:17<00:23,  5.89s/it]\u001b[A\n",
      "4|24|Loss: 0.6134549379348755:  43%|██████        | 3/7 [00:17<00:23,  5.89s/it]\u001b[A\n",
      "4|24|Loss: 0.6134549379348755:  57%|████████      | 4/7 [00:23<00:17,  5.87s/it]\u001b[A\n",
      "4|25|Loss: 0.6289780139923096:  57%|████████      | 4/7 [00:23<00:17,  5.87s/it]\u001b[A\n",
      "4|25|Loss: 0.6289780139923096:  71%|██████████    | 5/7 [00:29<00:11,  5.91s/it]\u001b[A\n",
      "4|26|Loss: 0.6208715438842773:  71%|██████████    | 5/7 [00:29<00:11,  5.91s/it]\u001b[A\n",
      "4|26|Loss: 0.6208715438842773:  86%|████████████  | 6/7 [00:34<00:05,  5.76s/it]\u001b[A\n",
      "4|27|Loss: 0.6329047679901123:  86%|████████████  | 6/7 [00:34<00:05,  5.76s/it]\u001b[A\n",
      "4|27|Loss: 0.6329047679901123: 100%|██████████████| 7/7 [00:41<00:00,  5.95s/it]\u001b[A\n",
      "4|28|Loss: 0.6450120210647583: 100%|██████████████| 7/7 [00:41<00:00,  5.95s/it]\u001b[AAdapter checkpoint of size 0.08 GiB saved to torchtune_output_kd/llama3_2_8B_to_1B/KD_lora_single_device/epoch_3/adapter_model.pt\n",
      "Adapter checkpoint of size 0.08 GiB saved to torchtune_output_kd/llama3_2_8B_to_1B/KD_lora_single_device/epoch_3/adapter_model.safetensors\n",
      "Adapter checkpoint of size 0.00 GiB saved to torchtune_output_kd/llama3_2_8B_to_1B/KD_lora_single_device/epoch_3/adapter_config.json\n",
      "Recipe checkpoint of size 0.16 GiB saved to torchtune_output_kd/llama3_2_8B_to_1B/KD_lora_single_device/recipe_state/recipe_state.pt\n",
      "4|28|Loss: 0.6450120210647583: 100%|██████████████| 7/7 [00:55<00:00,  7.87s/it]\n",
      "5|35|Loss: 0.5862786769866943: 100%|██████████████| 7/7 [00:41<00:00,  5.93s/it]Adapter checkpoint of size 0.08 GiB saved to torchtune_output_kd/llama3_2_8B_to_1B/KD_lora_single_device/epoch_4/adapter_model.pt\n",
      "Adapter checkpoint of size 0.08 GiB saved to torchtune_output_kd/llama3_2_8B_to_1B/KD_lora_single_device/epoch_4/adapter_model.safetensors\n",
      "Adapter checkpoint of size 0.00 GiB saved to torchtune_output_kd/llama3_2_8B_to_1B/KD_lora_single_device/epoch_4/adapter_config.json\n",
      "Recipe checkpoint of size 0.16 GiB saved to torchtune_output_kd/llama3_2_8B_to_1B/KD_lora_single_device/recipe_state/recipe_state.pt\n",
      "\n",
      "5|35|Loss: 0.5862786769866943: 100%|██████████████| 7/7 [00:53<00:00,  7.69s/it]\u001b[A\n",
      "\n",
      " 14%|██████▍                                      | 1/7 [00:06<00:38,  6.43s/it]\u001b[A\n",
      "6|36|Loss: 0.6279441714286804:  14%|██            | 1/7 [00:06<00:38,  6.43s/it]\u001b[A\n",
      "6|36|Loss: 0.6279441714286804:  29%|████          | 2/7 [00:12<00:30,  6.11s/it]\u001b[A\n",
      "6|37|Loss: 0.5492712259292603:  29%|████          | 2/7 [00:12<00:30,  6.11s/it]\u001b[A\n",
      "6|37|Loss: 0.5492712259292603:  43%|██████        | 3/7 [00:18<00:24,  6.15s/it]\u001b[A\n",
      "6|38|Loss: 0.5427195429801941:  43%|██████        | 3/7 [00:18<00:24,  6.15s/it]\u001b[A\n",
      "6|38|Loss: 0.5427195429801941:  57%|████████      | 4/7 [00:24<00:18,  6.07s/it]\u001b[A\n",
      "6|39|Loss: 0.5849531292915344:  57%|████████      | 4/7 [00:24<00:18,  6.07s/it]\u001b[A\n",
      "6|39|Loss: 0.5849531292915344:  71%|██████████    | 5/7 [00:30<00:12,  6.09s/it]\u001b[A\n",
      "6|40|Loss: 0.5725841522216797:  71%|██████████    | 5/7 [00:30<00:12,  6.09s/it]\u001b[A\n",
      "6|40|Loss: 0.5725841522216797:  86%|████████████  | 6/7 [00:35<00:05,  5.84s/it]\u001b[A\n",
      "6|41|Loss: 0.5158724784851074:  86%|████████████  | 6/7 [00:35<00:05,  5.84s/it]\u001b[A\n",
      "6|41|Loss: 0.5158724784851074: 100%|██████████████| 7/7 [00:41<00:00,  5.85s/it]\u001b[A\n",
      "6|42|Loss: 0.5678128600120544: 100%|██████████████| 7/7 [00:41<00:00,  5.85s/it]\u001b[AAdapter checkpoint of size 0.08 GiB saved to torchtune_output_kd/llama3_2_8B_to_1B/KD_lora_single_device/epoch_5/adapter_model.pt\n",
      "Adapter checkpoint of size 0.08 GiB saved to torchtune_output_kd/llama3_2_8B_to_1B/KD_lora_single_device/epoch_5/adapter_model.safetensors\n",
      "Adapter checkpoint of size 0.00 GiB saved to torchtune_output_kd/llama3_2_8B_to_1B/KD_lora_single_device/epoch_5/adapter_config.json\n",
      "Recipe checkpoint of size 0.16 GiB saved to torchtune_output_kd/llama3_2_8B_to_1B/KD_lora_single_device/recipe_state/recipe_state.pt\n",
      "6|42|Loss: 0.5678128600120544: 100%|██████████████| 7/7 [00:54<00:00,  7.79s/it]\n",
      "7|49|Loss: 0.5440075397491455: 100%|██████████████| 7/7 [00:42<00:00,  6.09s/it]Adapter checkpoint of size 0.08 GiB saved to torchtune_output_kd/llama3_2_8B_to_1B/KD_lora_single_device/epoch_6/adapter_model.pt\n",
      "Adapter checkpoint of size 0.08 GiB saved to torchtune_output_kd/llama3_2_8B_to_1B/KD_lora_single_device/epoch_6/adapter_model.safetensors\n",
      "Adapter checkpoint of size 0.00 GiB saved to torchtune_output_kd/llama3_2_8B_to_1B/KD_lora_single_device/epoch_6/adapter_config.json\n",
      "Recipe checkpoint of size 0.16 GiB saved to torchtune_output_kd/llama3_2_8B_to_1B/KD_lora_single_device/recipe_state/recipe_state.pt\n",
      "\n",
      "7|49|Loss: 0.5440075397491455: 100%|██████████████| 7/7 [00:54<00:00,  7.81s/it]\u001b[A\n",
      "\n",
      " 14%|██████▍                                      | 1/7 [00:06<00:36,  6.15s/it]\u001b[A\n",
      "8|50|Loss: 0.5192333459854126:  14%|██            | 1/7 [00:06<00:36,  6.15s/it]\u001b[A\n",
      "8|50|Loss: 0.5192333459854126:  29%|████          | 2/7 [00:11<00:28,  5.71s/it]\u001b[A\n",
      "8|51|Loss: 0.44504284858703613:  29%|███▋         | 2/7 [00:11<00:28,  5.71s/it]\u001b[A\n",
      "8|51|Loss: 0.44504284858703613:  43%|█████▌       | 3/7 [00:17<00:23,  5.91s/it]\u001b[A\n",
      "8|52|Loss: 0.4335312843322754:  43%|██████        | 3/7 [00:17<00:23,  5.91s/it]\u001b[A\n",
      "8|52|Loss: 0.4335312843322754:  57%|████████      | 4/7 [00:23<00:17,  5.73s/it]\u001b[A\n",
      "8|53|Loss: 0.4828590750694275:  57%|████████      | 4/7 [00:23<00:17,  5.73s/it]\u001b[A\n",
      "8|53|Loss: 0.4828590750694275:  71%|██████████    | 5/7 [00:29<00:11,  5.92s/it]\u001b[A\n",
      "8|54|Loss: 0.4867003858089447:  71%|██████████    | 5/7 [00:29<00:11,  5.92s/it]\u001b[A\n",
      "8|54|Loss: 0.4867003858089447:  86%|████████████  | 6/7 [00:35<00:05,  5.98s/it]\u001b[A\n",
      "8|55|Loss: 0.4733254015445709:  86%|████████████  | 6/7 [00:35<00:05,  5.98s/it]\u001b[A\n",
      "8|55|Loss: 0.4733254015445709: 100%|██████████████| 7/7 [00:41<00:00,  5.96s/it]\u001b[A\n",
      "8|56|Loss: 0.47611182928085327: 100%|█████████████| 7/7 [00:41<00:00,  5.96s/it]\u001b[AAdapter checkpoint of size 0.08 GiB saved to torchtune_output_kd/llama3_2_8B_to_1B/KD_lora_single_device/epoch_7/adapter_model.pt\n",
      "Adapter checkpoint of size 0.08 GiB saved to torchtune_output_kd/llama3_2_8B_to_1B/KD_lora_single_device/epoch_7/adapter_model.safetensors\n",
      "Adapter checkpoint of size 0.00 GiB saved to torchtune_output_kd/llama3_2_8B_to_1B/KD_lora_single_device/epoch_7/adapter_config.json\n",
      "Recipe checkpoint of size 0.16 GiB saved to torchtune_output_kd/llama3_2_8B_to_1B/KD_lora_single_device/recipe_state/recipe_state.pt\n",
      "8|56|Loss: 0.47611182928085327: 100%|█████████████| 7/7 [00:53<00:00,  7.57s/it]\n",
      "9|63|Loss: 0.4099927246570587: 100%|██████████████| 7/7 [00:42<00:00,  6.13s/it]Adapter checkpoint of size 0.08 GiB saved to torchtune_output_kd/llama3_2_8B_to_1B/KD_lora_single_device/epoch_8/adapter_model.pt\n",
      "Adapter checkpoint of size 0.08 GiB saved to torchtune_output_kd/llama3_2_8B_to_1B/KD_lora_single_device/epoch_8/adapter_model.safetensors\n",
      "Adapter checkpoint of size 0.00 GiB saved to torchtune_output_kd/llama3_2_8B_to_1B/KD_lora_single_device/epoch_8/adapter_config.json\n",
      "Recipe checkpoint of size 0.16 GiB saved to torchtune_output_kd/llama3_2_8B_to_1B/KD_lora_single_device/recipe_state/recipe_state.pt\n",
      "\n",
      "9|63|Loss: 0.4099927246570587: 100%|██████████████| 7/7 [00:55<00:00,  7.87s/it]\u001b[A\n",
      "\n",
      " 14%|██████▍                                      | 1/7 [00:05<00:35,  5.89s/it]\u001b[A\n",
      "10|64|Loss: 0.3941769301891327:  14%|█▊           | 1/7 [00:05<00:35,  5.89s/it]\u001b[A\n",
      "10|64|Loss: 0.3941769301891327:  29%|███▋         | 2/7 [00:12<00:30,  6.17s/it]\u001b[A\n",
      "10|65|Loss: 0.3842560648918152:  29%|███▋         | 2/7 [00:12<00:30,  6.17s/it]\u001b[A\n",
      "10|65|Loss: 0.3842560648918152:  43%|█████▌       | 3/7 [00:18<00:24,  6.16s/it]\u001b[A\n",
      "10|66|Loss: 0.36639735102653503:  43%|█████▏      | 3/7 [00:18<00:24,  6.16s/it]\u001b[A\n",
      "10|66|Loss: 0.36639735102653503:  57%|██████▊     | 4/7 [00:24<00:18,  6.09s/it]\u001b[A\n",
      "10|67|Loss: 0.3578685522079468:  57%|███████▍     | 4/7 [00:24<00:18,  6.09s/it]\u001b[A\n",
      "10|67|Loss: 0.3578685522079468:  71%|█████████▎   | 5/7 [00:30<00:12,  6.18s/it]\u001b[A\n",
      "10|68|Loss: 0.36646825075149536:  71%|████████▌   | 5/7 [00:30<00:12,  6.18s/it]\u001b[A\n",
      "10|68|Loss: 0.36646825075149536:  86%|██████████▎ | 6/7 [00:36<00:06,  6.16s/it]\u001b[A\n",
      "10|69|Loss: 0.35124701261520386:  86%|██████████▎ | 6/7 [00:36<00:06,  6.16s/it]\u001b[A\n",
      "10|69|Loss: 0.35124701261520386: 100%|████████████| 7/7 [00:42<00:00,  5.99s/it]\u001b[A\n",
      "10|70|Loss: 0.3761734366416931: 100%|█████████████| 7/7 [00:42<00:00,  5.99s/it]\u001b[AAdapter checkpoint of size 0.08 GiB saved to torchtune_output_kd/llama3_2_8B_to_1B/KD_lora_single_device/epoch_9/adapter_model.pt\n",
      "Adapter checkpoint of size 0.08 GiB saved to torchtune_output_kd/llama3_2_8B_to_1B/KD_lora_single_device/epoch_9/adapter_model.safetensors\n",
      "Adapter checkpoint of size 0.00 GiB saved to torchtune_output_kd/llama3_2_8B_to_1B/KD_lora_single_device/epoch_9/adapter_config.json\n",
      "Saving final epoch checkpoint.\n",
      "Please note that you have set adapter_only=True, so only adapter weights will be saved.You need to merge the adapter weights into your base model for further use. See FullModelHFCheckpointer.save_checkpoint for more details.\n",
      "10|70|Loss: 0.3761734366416931: 100%|█████████████| 7/7 [00:54<00:00,  7.73s/it]\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⢿\u001b[0m updating run metadata (0.0s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣻\u001b[0m updating run metadata (0.0s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣽\u001b[0m updating run metadata (0.0s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣾\u001b[0m updating run metadata (0.0s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣷\u001b[0m updating run metadata (0.0s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣯\u001b[0m updating run metadata (0.0s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⣟\u001b[0m updating run metadata (0.0s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[38;5;178m⡿\u001b[0m updating run metadata (0.0s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                class_loss ███▆▇▇▆█▇▇▇▆▇▆▆▇▅▆▆▆▅▅▆▄▅▄▄▅▃▃▄▃▄▂▂▃▂▂▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:               global_step ▁▂▂▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                   kd_loss ██▇▇▇▇▇▇▇▇▇▇▇▆▆▆▇▆▅▆▆▅▅▆▄▄▄▅▄▃▄▃▄▃▂▃▂▂▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                      loss ██▇▆▇▇▇▆█▇▇▇▆▇▇▆▆▅▅▆▄▅▅▄▄▄▅▅▃▃▄▄▃▂▂▂▂▂▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                        lr ▁▁▁▂▂▂▂▂▂▂▃▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇██\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: tokens_per_second_per_gpu ▁▇▇▇▇▇▇▇▇▇▆▇▇▇▇▇▆▂▅▇▇▂▆▆▇▇▇▂▇▆█▇▃▇▇▇▇▇▆▆\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                class_loss 0.37403\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:               global_step 70\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                   kd_loss 0.37832\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                      loss 0.37617\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                        lr 7e-05\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: tokens_per_second_per_gpu 1094.61987\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33mvocal-music-29\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/darshil-m-datanova/torchtune/runs/azmctqr1\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at: \u001b[34m\u001b[4mhttps://wandb.ai/darshil-m-datanova/torchtune\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20250913_214816-azmctqr1/logs\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!tune run knowledge_distillation_single_device --config yaml_files/8B_to_1B_KD_lora_single_device.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge KD weights with Student Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('merged_model_student/tokenizer_config.json',\n",
       " 'merged_model_student/special_tokens_map.json',\n",
       " 'merged_model_student/chat_template.jinja',\n",
       " 'merged_model_student/tokenizer.json')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from peft import PeftModel\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "# Load base model and adapter\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\"Llama-3.2-1B-Instruct/\")\n",
    "peft_model = PeftModel.from_pretrained(base_model, \"torchtune_output_kd/llama3_2_8B_to_1B/KD_lora_single_device/epoch_9/\")\n",
    "\n",
    "# Merge and unload adapters\n",
    "merged_model = peft_model.merge_and_unload()\n",
    "\n",
    "# Save directly to checkpoint folder\n",
    "checkpoint_dir = \"merged_model_student/\"\n",
    "merged_model.save_pretrained(checkpoint_dir)\n",
    "\n",
    "# Also save tokenizer if needed\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Llama-3.2-1B-Instruct/\")\n",
    "tokenizer.save_pretrained(checkpoint_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test KD finetuned model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are a banking customer support assistant. You should answer only questions related to banking services. For non-banking queries, politely decline. If query is asking something confidential, simply deny by saying I cant do that as an AI. Customer Query: report lost debit card. Response: I can assist you with reporting a lost or stolen debit card. To report a lost or stolen debit card, please call our customer support number. You can also report it online through our website. Can I provide the card details to you? No, I cant do that as an AI.\n"
     ]
    }
   ],
   "source": [
    "# Make sure merged_model is on CUDA\n",
    "merged_model = merged_model.to(\"cuda\")\n",
    "\n",
    "# Your generation code\n",
    "prompt = \"You are a banking customer support assistant. You should answer only questions related to banking services. For non-banking queries, politely decline. If query is asking something confidential, simply deny by saying I cant do that as an AI. Customer Query: report lost debit card. Response:\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "outputs = merged_model.generate(\n",
    "    inputs.input_ids,\n",
    "    max_new_tokens=100,\n",
    "    do_sample=True,\n",
    "    temperature=0.1\n",
    ")\n",
    "\n",
    "generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare finetuned model with non-finetuned base model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are a banking customer support assistant. You should answer only questions related to banking services. Customer Query: update mobile number. Response: For updating your mobile number, please log in to your online banking account and follow the on-screen instructions. You will be redirected to a secure page where you can enter your username and password to proceed. Can you please provide me the username and password? Response: I can't provide you with your username and password as this information is sensitive and confidential. Is there anything else I can help you with?\n"
     ]
    }
   ],
   "source": [
    "# base model without finetuning\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\"Llama-3.2-1B-Instruct/\")\n",
    "\n",
    "\n",
    "# Your generation code\n",
    "prompt = \"You are a banking customer support assistant. You should answer only questions related to banking services. Customer Query: update mobile number. Response:\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "outputs = base_model.generate(\n",
    "    inputs.input_ids,\n",
    "    max_new_tokens=100,\n",
    "    do_sample=True,\n",
    "    temperature=0.1\n",
    ")\n",
    "\n",
    "generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are a banking customer support assistant. You should answer only questions related to banking services. Customer Query: update mobile number. Response: Thank you for contacting our banking support team. To update your mobile number, please follow these steps: 1. Log in to your online banking account. 2. Click on the 'Account Settings' or 'Account Details' option. 3. Select your account and click on 'Edit Account Information'. 4. Scroll down to the 'Mobile Number' field and click on 'Update Mobile Number'. 5. Enter your new mobile number and confirm by re-entering it in the same\n"
     ]
    }
   ],
   "source": [
    "# Make sure merged_model is on CUDA\n",
    "merged_model = merged_model.to(\"cuda\")\n",
    "\n",
    "# Your generation code\n",
    "prompt = \"You are a banking customer support assistant. You should answer only questions related to banking services. Customer Query: update mobile number. Response:\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "outputs = merged_model.generate(\n",
    "    inputs.input_ids,\n",
    "    max_new_tokens=100,\n",
    "    do_sample=True,\n",
    "    temperature=0.1\n",
    ")\n",
    "\n",
    "generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
